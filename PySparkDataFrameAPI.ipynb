{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJoeN3e8_Gzk"
   },
   "source": [
    "# **Tratamento de dados com PySpark!**\n",
    "Aqui não estamos trabalhando com processamento em cluster, mas sim com o contexto Pyspark em um single node.<br>\n",
    "Então **cuidado** quando rodar ações pesadas como o collect(), por exemplo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Instalando JAVA_HOME e SPARK_HOME na máquina local - Anaconda + Windows 10**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você já tem o **JAVA** e **SPARK** instalados basta apontar para os diretórios específicos.<br>\n",
    "Obs.: *As linhas comentadas abaixo são uma opção para baixar / criar os diretórios do JAVA e SPARK para usar com esse notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tt7ZS1_wGgjn",
    "outputId": "285c94b7-3367-42e1-9964-0fe785079bbf"
   },
   "outputs": [],
   "source": [
    "#import time\n",
    "#!curl -O https://enos.itcollege.ee/~jpoial/allalaadimised/jdk8/jdk-8u291-linux-x64.tar.gz\n",
    "#time.sleepp(5)\n",
    "#!tar xf jdk-8u291-linux-x64.tar.gz\n",
    "#time.sleepp(5)\n",
    "#!curl -O http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
    "#time.sleepp(5)\n",
    "#!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
    "#time.sleep(5)\n",
    "#!pip install findspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aqui vamos criar as HOMEs necessárias importando o OS.**<br> \n",
    "**Recomenddo rodar a célula abaixo mesmo se JAVA_HOME e SPARK_HOME já estiverem cadastrados nas variaveis de ambiente.**<br>\n",
    "Obs.: Se você já tem o JAVA_HOME e SPARK_HOME configurados, basta ajustar os caminhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "sdOOq4twHN1K"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"jdk1.8.0_291\" \n",
    "os.environ[\"SPARK_HOME\"] = \"c:\\spark-2.3.1-bin-hadoop2.7\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Instalando JAVA_HOME e SPARK_HOME no Google Colab**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui vale lembrar que o Colab reseta após alu, então você terá que executar esse procedimento sempre que for trabalhar com o notebook.<br> \n",
    "Obs.: *No caso do colab o dataset tem que ser abixado sempre que o ambiente for resetado*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "#!apt-get update\n",
    "#!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "#time.sleepp(5)\n",
    "#!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
    "#time.sleepp(5)\n",
    "#!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
    "#time.sleepp(5)\n",
    "#!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Criando o SparkContext e SparkSession**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "id": "wjfF7LLgHZe3",
    "outputId": "1733f93c-ac9d-4a79-d52d-d426387d07ac"
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "Gs7fzvxcHfvw",
    "outputId": "c007285e-063d-4f7e-8dba-3d6b101a0f00"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "#spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:  #3498db ; color: white;\">\n",
    "<b> PySpark DataFrames</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ns7LEFl-rKQf"
   },
   "source": [
    "###  **Baixando o Dataset e criando o Dataframe!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retire os comentários das linhas abaixo apenas se você não tem o arquivo.\n",
    "Caso já tenha, deixe comentado. <br>\n",
    "Esse é um dataset de **registro de crimes da cidade de Chicago** de **2001 até hoje** e tem quase **2GB**<br>\n",
    "**Obs.:** *No caso do colab sempre lembrar se verificar se o arquivo ainda está disponível no ambiente.<br>\n",
    "*Voce pode fazer isso usando o comando abaixo em uma célula tipo code*<br>\n",
    "!ls -lh\n",
    "\n",
    "**IMPORTANTE** -> Foi feita extensa busca por dados similares de uma cidade específica do **Oregon** mas infelizmente os registros não foram encontrados. **◬**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "LpzHcELCPxOs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\f07699b\\Documents\\Residencia Dados\\Treinamentos\\Apache pyspark by example\\scripts\\rows.csv_accessType=DOWNLOAD\n",
      "        1 arquivo(s) movido(s).\n"
     ]
    }
   ],
   "source": [
    "#!curl -O https://data.cityofchicago.org/api/views/ijzp-q8t2/rows.csv?accessType=DOWNLOAD\n",
    "#time.sleepp(5)    \n",
    "\n",
    "#Se você estiver no colab (linux) vai renomear o arquivo usando o mv\n",
    "#!mv 'rows.csv?accessType=DOWNLOAD' tretas_de_chicago.csv\n",
    "\n",
    "#Se você estiver no windoes vai renomear o arquivo usando o move\n",
    "#!move \"rows.csv?accessType=DOWNLOAD\" tretas_de_chicago.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **read.csv()**\n",
    "#### Aqui nós importamos os dados do CSV para um DataFrame PysPark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "We2yp-o7Hgu9",
    "outputId": "b753a016-c8e2-4994-8b85-46666797306a"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col, lit\n",
    "rc = spark.read.csv('tretas_de_chicago.csv', header=True).withColumn('Date', to_timestamp(col('Date'),'MM/dd/yyy hh:mm:ss a')).filter(col('Date') <= lit('2018-11-11'))\n",
    "rc.show(5, truncate = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **take()** \n",
    "#### Retorna  o conteudo de linhas do dataframe. O numero que passamos como argumento a funão vai representar o número de linhas coletadas rc.head() tem exatamente a mesma saída de rc.take(), lembrando que esse head() aqui do contecto Spark não é parecido com o head() do pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.take(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **collect()**\n",
    "#### Coleta todos os dados do dataframe. Cuidado ao usar, pois pode cauisar um crash no driver node!\n",
    "#### Se após rodar o collect() acontecer esse problema com seu Jupter Noteboolk -->>  **Exception: Java gateway process exited before sending the driver its port number**, apague e descompacte novamente o diretório do **SPARK_HOME**\n",
    "#### Como isso é apenas um LAB, essa opção é possível, então apriveite!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vai testar?\n",
    "#rc.collect()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **show()**\n",
    "#### Vai printar 3 linhas do dataset incuindo o header.  Esse sim é igualzinho a saída do .head() do Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **count()**\n",
    "#### Vai contar quantos registros temos no Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O PySApark, com base nos dados define de forma automática o tipo de dados que está sendo importando. Porpem em situaçãoes de produção, é recomendado que o schema seja definido pelo usuário. Um exemplo são **datas** que na maioroia das vezes são importadas como **strings**<p>\n",
    "Para trabalhar com schemas precisamos importar algumas coisas antes da biblioteca **pyspark.sql.types**<br>\n",
    "**StructType**-->>Encapsula a estrutura do schema<br>\n",
    "**StructField**-->> É usado na definição de cada campo<br>\n",
    "**Type()**-->> Se refere ao tipo de campo. Pode ser **IntegerType**, **StringType**, **BooleanType**, etc... Acho que deu pra pegar a ideia.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DateType, FloatType, TimestampType, DoubleType\n",
    "#Obs.: Não vamos usar tudo isso, mas é bom saber que temos várias opções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **printSchema()**\n",
    "#### Usamos esse comando para ver o schema do dataframe. \n",
    "#### Baseado no output abaixo vemos que o campo \"Date\" é um timestamp, porém o campo \"Updated On\" que também é uma data, está como string. Precisamos ajustar isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Para ajustar as os campos usamos a sintaxe abaixo**.\n",
    "StrucType vai receber uma lista onde cada item é um StructField que recebe 3 argumentos<br>\n",
    "#### 1 - Coluna\n",
    "#### 2 - Filed Type\n",
    "#### 3 - Se o campo pode ter nulos ou não (True | False)\n",
    "Ex.:\n",
    "**rc_schema = StructType([StructField('ID', StringType, True),StructField('Case Number', StringType, True)])**\n",
    "\n",
    "Ps.: Devemos fazer a sequencia **StructField(Coluna, Type, True|False)** para TODAS as colunas. \n",
    "<p>\n",
    "**DICA** <br>\n",
    "Quando estiver trabalhando com o Schema de vários campos, a melhor coisa a fazer é tratar as colunas que são específicas primeiro.<br>\n",
    "Depois criar uma tratativa padrão para as demais.<br>\n",
    "No meu caso foi transformar elguns campos em Timestamp, Boolean e Double e depois todo o resto em string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = rc.columns\n",
    "labels\n",
    "for coluna in range(len(labels)):\n",
    "    if labels[coluna] == 'Date'or labels[coluna] == 'Updated On':\n",
    "        labels[coluna] = (labels[coluna], TimestampType(), True)\n",
    "    elif labels[coluna] == 'Arrest' or labels[coluna] == 'Domestic':\n",
    "        labels[coluna] = (labels[coluna], BooleanType(), True)\n",
    "    elif labels[coluna] == 'Year':\n",
    "        labels[coluna] = (labels[coluna], IntegerType(),True)\n",
    "    elif labels[coluna] =='Latitude' or labels[coluna] =='Longitude':\n",
    "        labels[coluna] = (labels[coluna], DoubleType(), True)\n",
    "    else: labels[coluna] = (labels[coluna], StringType(), True)\n",
    "      \n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui usamos uma **lambda function**, que vai passar por todos os items da lista labels e executar ação que adiciona os 3 valores \n",
    "de cada item daquela iteração na variavel screma. Os valores são: **Index 0** -> Coluna / **Index 1** -> field type / **Index 2** -> True ou False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField (x[0],x[1],x[2]) for x in labels])\n",
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Carregando o Schema**\n",
    "Aqui a gente pega o schema que foi criado e usa para carregar o CSV. Simples assim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_schema = spark.read.csv('tretas_de_chicago.csv', schema = schema, header = True)\n",
    "rc_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Trabalhando com Colunas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Selecionando colunas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos acessar as colunas em dataframe dem PySpark de duas maneiras:<p>\n",
    "Por indexing  -> **df['Column_name']**<br>\n",
    "Por função    -> **df.select(col('column_name'))** ou **df.select('column_name')**<p>\n",
    "É importante lembrar que se o nome da coluna tiver espaços ou nomes reservados você **não vai conseguir acessar** usando o acesso via atributo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select(col('ID')).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select('ID','Date','Arrest').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc['ID', 'Date','Arrest'].show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Trabalhando com os headers**\n",
    "Para retornar o header de um Dataframe em **PySpark** fazemos igual ao **Pandas**.<br>\n",
    "E como sa saída é uma lista, podemos acessar essa lista via index ou usar outras ações aplicaveis a listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vaocê tirar os brackets e testar as saídas\n",
    "rc.columns[0:3]\n",
    "#rc.columns\n",
    "#list(reversed(rc.columns))\n",
    "#rc.columns[::-1]\n",
    "#len(rc.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Adicionando novas colunas**\n",
    "\n",
    "Pandas -> **df['coluna_nova'] = df['coluna_velha'] * 2**<br>\n",
    "PySpark -> **df.withColumn('coluna_nova', 2 * df['coluna_velha']**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = rc.withColumn('coluna_nova_2', rc['ID'] / 2)\n",
    "rc.select('ID', 'coluna_nova_2').show(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Mudando os nomes das colunas**\n",
    "Pandas  -> **df.rename(columns={'Nome_antigo':'Nome_novo'})**<br>\n",
    "PySpark -> **df.withColumnRenamed('Nome_antigo','Nome_Novo')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = rc.withColumnRenamed('coluna_nova_2','IDx2')\n",
    "rc.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Removendo colunas**\n",
    "\n",
    "PySpark -> **df = df.drop('column')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = rc.drop('IDx2')\n",
    "rc.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Trabalhando Filtros e Linhas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **filter()**\n",
    "#### Diferente do Pandas, onde podemos filtrar direto na seleção da coluna, ex: df['coluna' > 50], em PySpark nós usamos a função filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.filter(col('Date')  > '2017-11-11' ).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **distinct()**\n",
    "#### Selecionando valores únicos em um dataframe\n",
    "#### No pandas usamos **df['coluna'].unique()**, já aqui é um pouco diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select('Arrest').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **count()**\n",
    "#### Com ele contamos os valores selecionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prisoes = rc.filter(col('Arrest') == True).count() \n",
    "print(prisoes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **orderBy()**\n",
    "#### Usamos para fazer a ordenação do dataframe de acordo com a coluna selecionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.filter(col('District') != 'null').select(col('District')).distinct().orderBy(col('District')).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **groupBy()**\n",
    "#### Usamos para agrupar valores de uma coluna específica e usar alguma agregação nesse resultado, como count(), sum(), entre outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.groupBy('Arrest').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **union()**\n",
    "#### **Concatenando Dataframes**\n",
    "#### Seguindo as premissas já conhecidas de Python, Dataframes são imutaveis, deste modo não podemos fazer um append como fazemos com listas. Neste caso devemos concatenar os ataframes uns con os outros. \n",
    "#### **Critérios para concatenação**\n",
    "Os Dataframes devem ter o mesmo numero de colunas<br>\n",
    "Os Dataframes devem ter o mesmo schema\n",
    "#### No Pandas nós usamnos **pd.concat(df,df2)**, no PySpark usamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para esse exemplo usaremso uma parte menor do Dataframe\n",
    "print(f\"Para o Distrito 008 temos {rc.filter(col('District') == '008').count()} registros\")\n",
    "print(f\"Para o Distrito 009 temos {rc.filter(col('District') == '009').count()} registros\")\n",
    "rc1 = rc.filter(col('District') == '008')\n",
    "rc2 = rc.filter(col('District') == '009')\n",
    "print(f\"Apos usar o union() o total de registros do novo dataframe é {rc1.union(rc2).count()}, que representa a soma dos totais anteriores.\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Desafios**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1) Quantos crimes resultaram em prisões?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Primeiro temos que entender o time de dados que estamos manipulando e suas variações.\n",
    "#Quantos valores possiveis temos para Arrest?\n",
    "'''\n",
    "rc.select(col('Arrest')).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Qual o tipo de dado?'''\n",
    "rc.printSchema()\n",
    "#Arrest: string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Agora fazemos a conta!'''\n",
    "result = round((rc.filter(col('Arrest') == 'true').count() / rc.select(col('Arrest')).count()) * 100,2) \n",
    "print(f'A porcentagem de crimes que resultaram em prisões é de {result}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1) Quais o TOP 3 de locais mais perigosos de Chigaco?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Primeiro vamos entender os dados. Temo um campo chamado Block (bairro)  que pode nos dar a resposta.\n",
    "#Mas antes algumas premissas devem ser definidas: O tipo de \"crime\" terá algum peso na resposta ou\n",
    "#apenas a quantidade de ocorrencias importa?\n",
    "É sempre importante fazer esse tipo de pergunta, mas nesse caso específico, vamos de quantidade.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos entender as informações que temos usando o **groupBy()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.groupBy('Block').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora que temos a lista dos blocks com a contagem dos crimes agregada precisamos penas odernar de acordo com nossa necessodade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.groupBy('Block').count().orderBy('count', ascending = False).show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3) Quais o TOP 3 bairros com maior número de prisoes efetuadas?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Essa foi fácil. Agora que sabemso como agrupar as informações com o groupBy() e fazer as agregações, basta filtar as informações.\n",
    "Aqui vamos primeiro filtrar todos os registros onde o canpo Arrest é iguala  True. Depois só precisamos fazer as agregações.\n",
    "'''\n",
    "\n",
    "rc.filter(col('Arrest') == True).groupBy('Block').count().orderBy('count', ascending = False).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"background-color:  #3498db ; color: white;\">\n",
    "<b> Essa é uma introdução básica de uso do DataFrame API do Pyspark.<br><br>\n",
    "Existe muito mais conteúdo por ai, especialmente no website do atual mantenedor do Spark, a Databricks.<br> \n",
    "Para mais informações vá até https://academy.databricks.com <br><br>\n",
    "Também recomendo os cursos do <b>Jonathan Fernandes</b>.<br> \n",
    "Ele tem ótimos cursos e foi de lá que tirei a deia de usar esse dataset específico de Chicago transferindo o ambiente Spark para a máquina local usando Anaconda.<br>\n",
    "</h5>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_environment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
