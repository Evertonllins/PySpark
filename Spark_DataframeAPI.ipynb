{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJoeN3e8_Gzk"
   },
   "source": [
    "# **Tratamento de dados com Pyspark!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dd6t0uFzuR4X"
   },
   "source": [
    "## Neste exemplo vamoscarregar uma sessão Pyspark no Anaconda, rodando em SO Windows 10.\n",
    "#### Aqui vamos considerar que o Anaconda está instalado rodando na sua máquina.\n",
    "-- Confirmando em qual env do CONDA você está --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "3ACYMwhgHTYz",
    "outputId": "fadb41ad-0e4e-40b7-b606-0a12d8dcf7af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\f07699b\\Anaconda3\\envs\\spark\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Instalando JAVA_HOME e SPARK_HOME!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tt7ZS1_wGgjn",
    "outputId": "285c94b7-3367-42e1-9964-0fe785079bbf"
   },
   "outputs": [],
   "source": [
    "#!curl -O https://enos.itcollege.ee/~jpoial/allalaadimised/jdk8/jdk-8u291-linux-x64.tar.gz\n",
    "#!tar xf jdk-8u291-linux-x64.tar.gz\n",
    "#!curl -O http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.3.1-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sdOOq4twHN1K"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"jre1.8.0_231\" \n",
    "os.environ[\"SPARK_HOME\"] = \"spark-2.3.1-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Criando o SparkContext e SparkSession**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "id": "wjfF7LLgHZe3",
    "outputId": "1733f93c-ac9d-4a79-d52d-d426387d07ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ABRGA1WL0020067.fiatauto.adfa.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "Gs7fzvxcHfvw",
    "outputId": "c007285e-063d-4f7e-8dba-3d6b101a0f00"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ABRGA1WL0020067.fiatauto.adfa.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x264e88f8848>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ns7LEFl-rKQf"
   },
   "source": [
    "###  **Baixando o Dataset e criando o Dataframe!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retire os comentários das linhas abaixo apenas se você não tem o arquivo.\n",
    "Caso já tenha, deixe comentado. <br>\n",
    "Esse é um dataset de **registro de crimes da cidade de Chicago** de **2001 até hoje** e tem quase **2GB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpzHcELCPxOs"
   },
   "outputs": [],
   "source": [
    "#!curl -O https://data.cityofchicago.org/api/views/ijzp-q8t2/rows.csv?accessType=DOWNLOAD\n",
    "#!mv 'rows.csv?accessType=DOWNLOAD' reported-crimes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "We2yp-o7Hgu9",
    "outputId": "b753a016-c8e2-4994-8b85-46666797306a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|      ID|Case Number|               Date|               Block|IUCR|Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|    Latitude|    Longitude|            Location|\n",
      "+--------+-----------+-------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|10224738|   HY411648|2015-09-05 13:30:00|     043XX S WOOD ST|0486|     BATTERY|DOMESTIC BATTERY ...|           RESIDENCE| false|    true|0924|     009|  12|            61|     08B|     1165074|     1875917|2015|02/10/2018 03:50:...|41.815117282|-87.669999562|(41.815117282, -8...|\n",
      "|10224739|   HY411615|2015-09-04 11:30:00| 008XX N CENTRAL AVE|0870|       THEFT|      POCKET-PICKING|             CTA BUS| false|   false|1511|     015|  29|            25|      06|     1138875|     1904869|2015|02/10/2018 03:50:...|41.895080471|-87.765400451|(41.895080471, -8...|\n",
      "|11646166|   JC213529|2018-09-01 00:01:00|082XX S INGLESIDE...|0810|       THEFT|           OVER $500|           RESIDENCE| false|    true|0631|     006|   8|            44|      06|        null|        null|2018|04/06/2019 04:04:...|        null|         null|                null|\n",
      "|10224740|   HY411595|2015-09-05 12:45:00|   035XX W BARRY AVE|2023|   NARCOTICS|POSS: HEROIN(BRN/...|            SIDEWALK|  true|   false|1412|     014|  35|            21|      18|     1152037|     1920384|2015|02/10/2018 03:50:...|41.937405765|-87.716649687|(41.937405765, -8...|\n",
      "|10224741|   HY411610|2015-09-05 13:00:00| 0000X N LARAMIE AVE|0560|     ASSAULT|              SIMPLE|           APARTMENT| false|    true|1522|     015|  28|            25|     08A|     1141706|     1900086|2015|02/10/2018 03:50:...|41.881903443|-87.755121152|(41.881903443, -8...|\n",
      "+--------+-----------+-------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col, lit\n",
    "rc = spark.read.csv('reported-crimes.csv', header=True).withColumn('Date', to_timestamp(col('Date'),'MM/dd/yyy hh:mm:ss a')).filter(col('Date') <= lit('2018-11-11'))\n",
    "rc.show(5, truncate = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dando aquela olhada nos dados!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **take()** \n",
    "#### Retorna  o conteudo de linhas do dataframe. O numero que passamos como argumento a funão vai representar o númro de linhas coletadas rc.head() tem exatamente a mesma saída de rc.take(), lembrando que esse head() aqui do contecto Spark não é parecido com o head() do pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID='10224738', Case Number='HY411648', Date=datetime.datetime(2015, 9, 5, 13, 30), Block='043XX S WOOD ST', IUCR='0486', Primary Type='BATTERY', Description='DOMESTIC BATTERY SIMPLE', Location Description='RESIDENCE', Arrest='false', Domestic='true', Beat='0924', District='009', Ward='12', Community Area='61', FBI Code='08B', X Coordinate='1165074', Y Coordinate='1875917', Year='2015', Updated On='02/10/2018 03:50:01 PM', Latitude='41.815117282', Longitude='-87.669999562', Location='(41.815117282, -87.669999562)')]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.take(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **collect()**\n",
    "#### Coleta todos os dados do dataframe. Cuidado ao usar, pois pode cauisar um crash no driver node!\n",
    "#### Se após rodar o collect() acontecer esse problema com seu Jupter Noteboolk -->>  **Exception: Java gateway process exited before sending the driver its port number**, apague e descompacte novamente o diretório do **SPARK_HOME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vai testar?\n",
    "#rc.collect()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **show()**\n",
    "#### Vai printar 3 linhas do dataset incuindo o header.  Esse sim é igualzinho a saída do .head() do Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|      ID|Case Number|               Date|               Block|IUCR|Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|    Latitude|    Longitude|            Location|\n",
      "+--------+-----------+-------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|10224738|   HY411648|2015-09-05 13:30:00|     043XX S WOOD ST|0486|     BATTERY|DOMESTIC BATTERY ...|           RESIDENCE| false|    true|0924|     009|  12|            61|     08B|     1165074|     1875917|2015|02/10/2018 03:50:...|41.815117282|-87.669999562|(41.815117282, -8...|\n",
      "|10224739|   HY411615|2015-09-04 11:30:00| 008XX N CENTRAL AVE|0870|       THEFT|      POCKET-PICKING|             CTA BUS| false|   false|1511|     015|  29|            25|      06|     1138875|     1904869|2015|02/10/2018 03:50:...|41.895080471|-87.765400451|(41.895080471, -8...|\n",
      "|11646166|   JC213529|2018-09-01 00:01:00|082XX S INGLESIDE...|0810|       THEFT|           OVER $500|           RESIDENCE| false|    true|0631|     006|   8|            44|      06|        null|        null|2018|04/06/2019 04:04:...|        null|         null|                null|\n",
      "+--------+-----------+-------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Pyspark, com base nos dados define de forma automática o tipo de dados que está sendo importando. Porpem em situaçãoes de produção, é recomendado que o schema seja definido pelo usuário. Um exemplo são **datas** que na maioroia das vezes são importadas como **strings**<p>\n",
    "Para trabalhar com schemas precisamos importar algumas coisas antes da biblioteca **pyspark.sql.types**<br>\n",
    "**StructType**-->>Encapsula a estrutura do schema<br>\n",
    "**StructField**-->> É usado na definição de cada campo<br>\n",
    "**Type()**-->> Se refere ao tipo de campo. Pode ser **IntegerType**, **StringType**, **BooleanType**, etc... Acho que deu pra pegar a ideia.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DateType, FloatType, TimestampType, DoubleType\n",
    "#Obs.: Não vamos usar tudo isso, mas é bom saber que temos várias opções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **printSchema()**\n",
    "#### Usamos esse comando para ver o schema do dataframe. \n",
    "#### Baseado no output abaixo vemos que o campo \"Date\" é um timestamp, porém o campo \"Updated On\" que também é uma data, está como string. Precisamos ajustar isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Case Number: string (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Block: string (nullable = true)\n",
      " |-- IUCR: string (nullable = true)\n",
      " |-- Primary Type: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Location Description: string (nullable = true)\n",
      " |-- Arrest: string (nullable = true)\n",
      " |-- Domestic: string (nullable = true)\n",
      " |-- Beat: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- Ward: string (nullable = true)\n",
      " |-- Community Area: string (nullable = true)\n",
      " |-- FBI Code: string (nullable = true)\n",
      " |-- X Coordinate: string (nullable = true)\n",
      " |-- Y Coordinate: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Updated On: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Para ajustar as os campos usamos a sintaxe abaixo**.\n",
    "StrucType vai receber uma lista onde cada item é um StructField que recebe 3 argumentos<br>\n",
    "#1 - Coluna<br>\n",
    "#2 - Filed Type<br>\n",
    "#3 - Se o campo pode ter nulos ou não (True | False)<br>\n",
    "Ex.:<br>\n",
    "**rc_schema = StructType([StructField('ID', StringType, True),StructField('Case Number', StringType, True)])**\n",
    "\n",
    "Ps.: Devemos fazer a sequencia **StructField(Coluna, Type, True|False)** para TODAS as colunas. (╬ಠ益ಠ)\n",
    "<p>\n",
    "**DICA**<br>\n",
    "Quando estiver trabalhando com vários campos, a melhor coisa a fazer é tratar as colunas que são específicas primeiro.<br>\n",
    "Depois criar uma tratativa padrão para as demais.<br>\n",
    "No meu caso foi transformar elguns campos em Timestamp, Boolean e Double e depois todo o resto en string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ID', StringType, True), ('Case Number', StringType, True), ('Date', TimestampType, True), ('Block', StringType, True), ('IUCR', StringType, True), ('Primary Type', StringType, True), ('Description', StringType, True), ('Location Description', StringType, True), ('Arrest', BooleanType, True), ('Domestic', BooleanType, True), ('Beat', StringType, True), ('District', StringType, True), ('Ward', StringType, True), ('Community Area', StringType, True), ('FBI Code', StringType, True), ('X Coordinate', StringType, True), ('Y Coordinate', StringType, True), ('Year', IntegerType, True), ('Updated On', TimestampType, True), ('Latitude', DoubleType, True), ('Longitude', DoubleType, True), ('Location', StringType, True)]\n"
     ]
    }
   ],
   "source": [
    "labels = rc.columns\n",
    "labels\n",
    "for coluna in range(len(labels)):\n",
    "    if labels[coluna] == 'Date'or labels[coluna] == 'Updated On':\n",
    "        labels[coluna] = (labels[coluna], TimestampType(), True)\n",
    "    elif labels[coluna] == 'Arrest' or labels[coluna] == 'Domestic':\n",
    "        labels[coluna] = (labels[coluna], BooleanType(), True)\n",
    "    elif labels[coluna] == 'Year':\n",
    "        labels[coluna] = (labels[coluna], IntegerType(),True)\n",
    "    elif labels[coluna] =='Latitude' or labels[coluna] =='Longitude':\n",
    "        labels[coluna] = (labels[coluna], DoubleType(), True)\n",
    "    else: labels[coluna] = (labels[coluna], StringType(), True)\n",
    "      \n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui usamos uma **lambda function**, que vai passar por todos os items da lista labels e executar ação que adiciona os 3 valores \n",
    "de cada item daquela iteração na variavel screma. Os valores são: **Index 0** -> Coluna / **Index 1** -> field type / **Index 2** -> True ou False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(ID,StringType,true),StructField(Case Number,StringType,true),StructField(Date,TimestampType,true),StructField(Block,StringType,true),StructField(IUCR,StringType,true),StructField(Primary Type,StringType,true),StructField(Description,StringType,true),StructField(Location Description,StringType,true),StructField(Arrest,BooleanType,true),StructField(Domestic,BooleanType,true),StructField(Beat,StringType,true),StructField(District,StringType,true),StructField(Ward,StringType,true),StructField(Community Area,StringType,true),StructField(FBI Code,StringType,true),StructField(X Coordinate,StringType,true),StructField(Y Coordinate,StringType,true),StructField(Year,IntegerType,true),StructField(Updated On,TimestampType,true),StructField(Latitude,DoubleType,true),StructField(Longitude,DoubleType,true),StructField(Location,StringType,true)))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = StructType([StructField (x[0],x[1],x[2]) for x in labels])\n",
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Carregando o Schema**\n",
    "Aqui a gente pega o schema que foi criado e usa para carregar o CSV. Simples assim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Case Number: string (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Block: string (nullable = true)\n",
      " |-- IUCR: string (nullable = true)\n",
      " |-- Primary Type: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Location Description: string (nullable = true)\n",
      " |-- Arrest: boolean (nullable = true)\n",
      " |-- Domestic: boolean (nullable = true)\n",
      " |-- Beat: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- Ward: string (nullable = true)\n",
      " |-- Community Area: string (nullable = true)\n",
      " |-- FBI Code: string (nullable = true)\n",
      " |-- X Coordinate: string (nullable = true)\n",
      " |-- Y Coordinate: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Updated On: timestamp (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc_schema = spark.read.csv('reported-crimes.csv', schema = schema, header = True)\n",
    "rc_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Trabalhando com Colunas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Selecionando colunas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos acessar as colunas em dataframe dem Pyspark de duas maneiras:<p>\n",
    "Por indexing  -> **df['Column_name']**<br>\n",
    "Por função    -> **df.select('column_name')**<p>\n",
    "É importante lembrar que se o nome da coluna tiver espaços ou nomes reservados você **não vai conseguir acessar** usando o acesso via atributo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+------+\n",
      "|      ID|               Date|Arrest|\n",
      "+--------+-------------------+------+\n",
      "|10224738|2015-09-05 13:30:00| false|\n",
      "|10224739|2015-09-04 11:30:00| false|\n",
      "|11646166|2018-09-01 00:01:00| false|\n",
      "|10224740|2015-09-05 12:45:00|  true|\n",
      "|10224741|2015-09-05 13:00:00| false|\n",
      "+--------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.select('ID','Date','Arrest').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+------+\n",
      "|      ID|               Date|Arrest|\n",
      "+--------+-------------------+------+\n",
      "|10224738|2015-09-05 13:30:00| false|\n",
      "|10224739|2015-09-04 11:30:00| false|\n",
      "|11646166|2018-09-01 00:01:00| false|\n",
      "|10224740|2015-09-05 12:45:00|  true|\n",
      "|10224741|2015-09-05 13:00:00| false|\n",
      "+--------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc['ID', 'Date','Arrest'].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Trabalhando com os headers**\n",
    "Para retornar o header de um Dataframe em **Pyspark** fazemos igual ao **Pandas**.<br>\n",
    "E como sa saída é uma lista, podemos acessar essa lista via index ou usar outras ações aplicaveis a listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vaocê tirar os brackets e testar as saídas\n",
    "rc.columns[0:3]\n",
    "#rc.columns\n",
    "#list(reversed(rc.columns))\n",
    "#rc.columns[::-1]\n",
    "#len(rc.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Adicionando novas colunas**\n",
    "\n",
    "Pandas -> **df['coluna_nova'] = df['coluna_velha'] * 2**<br>\n",
    "Pyspark -> **df.withColumn('coluna_nova', 2 * df['coluna_velha']**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = rc.withColumn('coluna_nova_2', rc['ID'] / 2)\n",
    "rc.select('ID', 'coluna_nova_2').show(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Removendo colunas**\n",
    "\n",
    "Pyspark -> **df = df.drop('column')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = rc.drop('coluna_nova_2')\n",
    "rc.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Trabalhando Filtros e Linhas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **filter()**\n",
    "#### Diferente do Pandas, onde podemos filtrar direto na seleção da coluna, ex: df['coluna' > 50], em pyspark nós usamos a função filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+--------------------+----+------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+--------+---------+--------+\n",
      "|      ID|Case Number|               Date|               Block|IUCR|      Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|Latitude|Longitude|Location|\n",
      "+--------+-----------+-------------------+--------------------+----+------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+--------+---------+--------+\n",
      "|11646166|   JC213529|2018-09-01 00:01:00|082XX S INGLESIDE...|0810|             THEFT|           OVER $500|           RESIDENCE| false|    true|0631|     006|   8|            44|      06|        null|        null|2018|04/06/2019 04:04:...|    null|     null|    null|\n",
      "|11645648|   JC212959|2018-01-01 08:00:00| 024XX N MONITOR AVE|1153|DECEPTIVE PRACTICE|FINANCIAL IDENTIT...|           RESIDENCE| false|   false|2515|     025|  30|            19|      11|        null|        null|2018|04/06/2019 04:04:...|    null|     null|    null|\n",
      "+--------+-----------+-------------------+--------------------+----+------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+--------+---------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.filter(col('Date')  > '2017-11-11' ).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **distinct()**\n",
    "#### Selecionando valores únicos em um dataframe\n",
    "#### No pandas usamos **df['coluna'].unique()**, já aqui é um pouco diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Arrest|\n",
      "+------+\n",
      "| false|\n",
      "|  true|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.select('Arrest').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **count()**\n",
    "#### Com ele contamos os valores selecionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1874411\n"
     ]
    }
   ],
   "source": [
    "prisoes = rc.filter(col('Arrest') == True).count() \n",
    "print(prisoes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **orderBy()**\n",
    "#### Usamos para fazer a ordenação do dataframe de acordo com a coluna selecionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|District|\n",
      "+--------+\n",
      "|     001|\n",
      "|     002|\n",
      "|     003|\n",
      "|     004|\n",
      "|     005|\n",
      "|     006|\n",
      "|     007|\n",
      "|     008|\n",
      "|     009|\n",
      "|     010|\n",
      "|     011|\n",
      "|     012|\n",
      "|     014|\n",
      "|     015|\n",
      "|     016|\n",
      "|     017|\n",
      "|     018|\n",
      "|     019|\n",
      "|     020|\n",
      "|     021|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.filter(col('District') != 'null').select(col('District')).distinct().orderBy(col('District')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **union()**\n",
    "#### **Concatenando Dataframes**\n",
    "#### Seguindo as premissas já conhecidas de Python, Dataframes são imutaveis, deste modo não podemos fazer um append como fazemos com listas. Neste caso devemos concatenar os ataframes uns con os outros. \n",
    "#### **Critérios para concatenação**\n",
    "Os Dataframes devem ter o mesmo numero de colunas<br>\n",
    "Os Dataframes devem ter o mesmo schema\n",
    "#### No Pandas nós usamnos **pd.concat(df,df2)**, no Pyspark usamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para o Distrito 008 temos 459542 registros\n",
      "Para o Distrito 009 temos 335902 registros\n",
      "Apos usar o union() o total de registros do novo dataframe é 795444, que representa a soma dos totais anteriores.\n"
     ]
    }
   ],
   "source": [
    "#Para esse exemplo usaremso uma parte menor do Dataframe\n",
    "print(f\"Para o Distrito 008 temos {rc.filter(col('District') == '008').count()} registros\")\n",
    "print(f\"Para o Distrito 009 temos {rc.filter(col('District') == '009').count()} registros\")\n",
    "rc1 = rc.filter(col('District') == '008')\n",
    "rc2 = rc.filter(col('District') == '009')\n",
    "print(f\"Apos usar o union() o total de registros do novo dataframe é {rc1.union(rc2).count()}, que representa a soma dos totais anteriores.\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_environment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
