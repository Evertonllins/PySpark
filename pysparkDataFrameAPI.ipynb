{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"background-color:  #3498db ; color: white;\">\n",
    "<b> Essa é uma introdução básica de uso do DataFrame API do Pyspark.<br><br>\n",
    "Existe muito mais conteúdo por ai, especialmente no website do atual mantenedor do Spark, a Databricks.<br> \n",
    "Para mais informações vá até https://academy.databricks.com <br><br>\n",
    "Também recomendo os cursos do <b>Jonathan Fernandes</b>.<br> \n",
    "Ele tem ótimos cursos e foi de lá que tirei a deia de usar esse dataset específico de Chicago transferindo o ambiente Spark para a máquina local usando Anaconda + Jupter.<br>\n",
    "Aqui não estamos trabalhando com o envio de processamento em cluster, mas sim com o contexto Pyspark.<br>\n",
    "Então **cuidado** quando rodar ações pesadas como o collect(), por exemplo. \n",
    "</h5>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJoeN3e8_Gzk"
   },
   "source": [
    "# **Tratamento de dados com Pyspark!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dd6t0uFzuR4X"
   },
   "source": [
    "## Neste exemplo vamoscarregar uma sessão Pyspark no Anaconda, rodando em SO Windows 10.\n",
    "#### Aqui vamos considerar que o Anaconda está instalado rodando sem problemas na sua máquina.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "3ACYMwhgHTYz",
    "outputId": "fadb41ad-0e4e-40b7-b606-0a12d8dcf7af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\f07699b\\Anaconda3\\envs\\spark\\python.exe\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Primeiro vamos confirmar em qual env do CONDA você está.\n",
    "Recomendo criar uma específica para trabalhar com o Spark.\n",
    "Essa recomendação é mais por organização que uma necesidade técnica, ficaa seu critério.\n",
    "'''\n",
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Instalando JAVA_HOME e SPARK_HOME!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você já tem o **JAVA** e **SPARK** instalados basta apontar para os diretórios específicos.<br>\n",
    "**As linhas comentadas abaixo são uma opção para baixar / criar os diretórios do JAVA e SPARK para usar com esse notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tt7ZS1_wGgjn",
    "outputId": "285c94b7-3367-42e1-9964-0fe785079bbf"
   },
   "outputs": [],
   "source": [
    "#!curl -O https://enos.itcollege.ee/~jpoial/allalaadimised/jdk8/jdk-8u291-linux-x64.tar.gz\n",
    "#!tar xf jdk-8u291-linux-x64.tar.gz\n",
    "#!curl -O http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
    "#!tar xf spark-2.3.1-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aqui vamos criar as HOMEs necessárias importando o OS.**<br> \n",
    "**Recomenddo rodar a célula abaixo mesmo se JAVA_HOME e SPARK_HOME já estiverem nas variaveis de ambiente.**<br>\n",
    "Obs.: Se você já tem o JAVA_HOME e SPARK_HOME configurados, basta ajustar os caminhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "sdOOq4twHN1K"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"jdk1.8.0_291\" \n",
    "os.environ[\"SPARK_HOME\"] = \"spark-2.3.1-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Criando o SparkContext e SparkSession**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "id": "wjfF7LLgHZe3",
    "outputId": "1733f93c-ac9d-4a79-d52d-d426387d07ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ABRGA1WL0020067.fiatauto.adfa.local:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "Gs7fzvxcHfvw",
    "outputId": "c007285e-063d-4f7e-8dba-3d6b101a0f00"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ABRGA1WL0020067.fiatauto.adfa.local:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x25bcc701688>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ns7LEFl-rKQf"
   },
   "source": [
    "###  **Baixando o Dataset e criando o Dataframe!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retire os comentários das linhas abaixo apenas se você não tem o arquivo.\n",
    "Caso já tenha, deixe comentado. <br>\n",
    "Esse é um dataset de **registro de crimes da cidade de Chicago** de **2001 até hoje** e tem quase **2GB**\n",
    "\n",
    "**IMPORTANTE** -> Foi feita extensa busca por dados similares de uma cidade específica do **Oregon** mas infelizmente os registros não foram encontrados. **◬**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpzHcELCPxOs"
   },
   "outputs": [],
   "source": [
    "#!curl -O https://data.cityofchicago.org/api/views/ijzp-q8t2/rows.csv?accessType=DOWNLOAD\n",
    "#!mv 'rows.csv?accessType=DOWNLOAD' reported-crimes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "We2yp-o7Hgu9",
    "outputId": "b753a016-c8e2-4994-8b85-46666797306a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|      ID|Case Number|               Date|               Block|IUCR|Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|    Latitude|    Longitude|            Location|\n",
      "+--------+-----------+-------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|10224738|   HY411648|2015-09-05 13:30:00|     043XX S WOOD ST|0486|     BATTERY|DOMESTIC BATTERY ...|           RESIDENCE| false|    true|0924|     009|  12|            61|     08B|     1165074|     1875917|2015|02/10/2018 03:50:...|41.815117282|-87.669999562|(41.815117282, -8...|\n",
      "|10224739|   HY411615|2015-09-04 11:30:00| 008XX N CENTRAL AVE|0870|       THEFT|      POCKET-PICKING|             CTA BUS| false|   false|1511|     015|  29|            25|      06|     1138875|     1904869|2015|02/10/2018 03:50:...|41.895080471|-87.765400451|(41.895080471, -8...|\n",
      "|11646166|   JC213529|2018-09-01 00:01:00|082XX S INGLESIDE...|0810|       THEFT|           OVER $500|           RESIDENCE| false|    true|0631|     006|   8|            44|      06|        null|        null|2018|04/06/2019 04:04:...|        null|         null|                null|\n",
      "|10224740|   HY411595|2015-09-05 12:45:00|   035XX W BARRY AVE|2023|   NARCOTICS|POSS: HEROIN(BRN/...|            SIDEWALK|  true|   false|1412|     014|  35|            21|      18|     1152037|     1920384|2015|02/10/2018 03:50:...|41.937405765|-87.716649687|(41.937405765, -8...|\n",
      "|10224741|   HY411610|2015-09-05 13:00:00| 0000X N LARAMIE AVE|0560|     ASSAULT|              SIMPLE|           APARTMENT| false|    true|1522|     015|  28|            25|     08A|     1141706|     1900086|2015|02/10/2018 03:50:...|41.881903443|-87.755121152|(41.881903443, -8...|\n",
      "+--------+-----------+-------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col, lit\n",
    "rc = spark.read.csv('reported-crimes.csv', header=True).withColumn('Date', to_timestamp(col('Date'),'MM/dd/yyy hh:mm:ss a')).filter(col('Date') <= lit('2018-11-11'))\n",
    "rc.show(5, truncate = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dando aquela olhada nos dados!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **take()** \n",
    "#### Retorna  o conteudo de linhas do dataframe. O numero que passamos como argumento a funão vai representar o númro de linhas coletadas rc.head() tem exatamente a mesma saída de rc.take(), lembrando que esse head() aqui do contecto Spark não é parecido com o head() do pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID='10224738', Case Number='HY411648', Date=datetime.datetime(2015, 9, 5, 13, 30), Block='043XX S WOOD ST', IUCR='0486', Primary Type='BATTERY', Description='DOMESTIC BATTERY SIMPLE', Location Description='RESIDENCE', Arrest='false', Domestic='true', Beat='0924', District='009', Ward='12', Community Area='61', FBI Code='08B', X Coordinate='1165074', Y Coordinate='1875917', Year='2015', Updated On='02/10/2018 03:50:01 PM', Latitude='41.815117282', Longitude='-87.669999562', Location='(41.815117282, -87.669999562)')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.take(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **collect()**\n",
    "#### Coleta todos os dados do dataframe. Cuidado ao usar, pois pode cauisar um crash no driver node!\n",
    "#### Se após rodar o collect() acontecer esse problema com seu Jupter Noteboolk -->>  **Exception: Java gateway process exited before sending the driver its port number**, apague e descompacte novamente o diretório do **SPARK_HOME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vai testar?\n",
    "#rc.collect()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **show()**\n",
    "#### Vai printar 3 linhas do dataset incuindo o header.  Esse sim é igualzinho a saída do .head() do Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|      ID|Case Number|               Date|               Block|IUCR|Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|    Latitude|    Longitude|            Location|\n",
      "+--------+-----------+-------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|10224738|   HY411648|2015-09-05 13:30:00|     043XX S WOOD ST|0486|     BATTERY|DOMESTIC BATTERY ...|           RESIDENCE| false|    true|0924|     009|  12|            61|     08B|     1165074|     1875917|2015|02/10/2018 03:50:...|41.815117282|-87.669999562|(41.815117282, -8...|\n",
      "|10224739|   HY411615|2015-09-04 11:30:00| 008XX N CENTRAL AVE|0870|       THEFT|      POCKET-PICKING|             CTA BUS| false|   false|1511|     015|  29|            25|      06|     1138875|     1904869|2015|02/10/2018 03:50:...|41.895080471|-87.765400451|(41.895080471, -8...|\n",
      "|11646166|   JC213529|2018-09-01 00:01:00|082XX S INGLESIDE...|0810|       THEFT|           OVER $500|           RESIDENCE| false|    true|0631|     006|   8|            44|      06|        null|        null|2018|04/06/2019 04:04:...|        null|         null|                null|\n",
      "+--------+-----------+-------------------+--------------------+----+------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **count()**\n",
    "#### Vai contar quantos registros temos no Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6753281"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Pyspark, com base nos dados define de forma automática o tipo de dados que está sendo importando. Porpem em situaçãoes de produção, é recomendado que o schema seja definido pelo usuário. Um exemplo são **datas** que na maioroia das vezes são importadas como **strings**<p>\n",
    "Para trabalhar com schemas precisamos importar algumas coisas antes da biblioteca **pyspark.sql.types**<br>\n",
    "**StructType**-->>Encapsula a estrutura do schema<br>\n",
    "**StructField**-->> É usado na definição de cada campo<br>\n",
    "**Type()**-->> Se refere ao tipo de campo. Pode ser **IntegerType**, **StringType**, **BooleanType**, etc... Acho que deu pra pegar a ideia.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DateType, FloatType, TimestampType, DoubleType\n",
    "#Obs.: Não vamos usar tudo isso, mas é bom saber que temos várias opções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **printSchema()**\n",
    "#### Usamos esse comando para ver o schema do dataframe. \n",
    "#### Baseado no output abaixo vemos que o campo \"Date\" é um timestamp, porém o campo \"Updated On\" que também é uma data, está como string. Precisamos ajustar isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Case Number: string (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Block: string (nullable = true)\n",
      " |-- IUCR: string (nullable = true)\n",
      " |-- Primary Type: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Location Description: string (nullable = true)\n",
      " |-- Arrest: string (nullable = true)\n",
      " |-- Domestic: string (nullable = true)\n",
      " |-- Beat: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- Ward: string (nullable = true)\n",
      " |-- Community Area: string (nullable = true)\n",
      " |-- FBI Code: string (nullable = true)\n",
      " |-- X Coordinate: string (nullable = true)\n",
      " |-- Y Coordinate: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Updated On: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Para ajustar as os campos usamos a sintaxe abaixo**.\n",
    "StrucType vai receber uma lista onde cada item é um StructField que recebe 3 argumentos<br>\n",
    "#### 1 - Coluna\n",
    "#### 2 - Filed Type\n",
    "#### 3 - Se o campo pode ter nulos ou não (True | False)\n",
    "Ex.:\n",
    "**rc_schema = StructType([StructField('ID', StringType, True),StructField('Case Number', StringType, True)])**\n",
    "\n",
    "Ps.: Devemos fazer a sequencia **StructField(Coluna, Type, True|False)** para TODAS as colunas. \n",
    "<p>\n",
    "**DICA** <br>\n",
    "Quando estiver trabalhando com o Schema de vários campos, a melhor coisa a fazer é tratar as colunas que são específicas primeiro.<br>\n",
    "Depois criar uma tratativa padrão para as demais.<br>\n",
    "No meu caso foi transformar elguns campos em Timestamp, Boolean e Double e depois todo o resto en string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ID', StringType, True), ('Case Number', StringType, True), ('Date', TimestampType, True), ('Block', StringType, True), ('IUCR', StringType, True), ('Primary Type', StringType, True), ('Description', StringType, True), ('Location Description', StringType, True), ('Arrest', BooleanType, True), ('Domestic', BooleanType, True), ('Beat', StringType, True), ('District', StringType, True), ('Ward', StringType, True), ('Community Area', StringType, True), ('FBI Code', StringType, True), ('X Coordinate', StringType, True), ('Y Coordinate', StringType, True), ('Year', IntegerType, True), ('Updated On', TimestampType, True), ('Latitude', DoubleType, True), ('Longitude', DoubleType, True), ('Location', StringType, True)]\n"
     ]
    }
   ],
   "source": [
    "labels = rc.columns\n",
    "labels\n",
    "for coluna in range(len(labels)):\n",
    "    if labels[coluna] == 'Date'or labels[coluna] == 'Updated On':\n",
    "        labels[coluna] = (labels[coluna], TimestampType(), True)\n",
    "    elif labels[coluna] == 'Arrest' or labels[coluna] == 'Domestic':\n",
    "        labels[coluna] = (labels[coluna], BooleanType(), True)\n",
    "    elif labels[coluna] == 'Year':\n",
    "        labels[coluna] = (labels[coluna], IntegerType(),True)\n",
    "    elif labels[coluna] =='Latitude' or labels[coluna] =='Longitude':\n",
    "        labels[coluna] = (labels[coluna], DoubleType(), True)\n",
    "    else: labels[coluna] = (labels[coluna], StringType(), True)\n",
    "      \n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui usamos uma **lambda function**, que vai passar por todos os items da lista labels e executar ação que adiciona os 3 valores \n",
    "de cada item daquela iteração na variavel screma. Os valores são: **Index 0** -> Coluna / **Index 1** -> field type / **Index 2** -> True ou False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(ID,StringType,true),StructField(Case Number,StringType,true),StructField(Date,TimestampType,true),StructField(Block,StringType,true),StructField(IUCR,StringType,true),StructField(Primary Type,StringType,true),StructField(Description,StringType,true),StructField(Location Description,StringType,true),StructField(Arrest,BooleanType,true),StructField(Domestic,BooleanType,true),StructField(Beat,StringType,true),StructField(District,StringType,true),StructField(Ward,StringType,true),StructField(Community Area,StringType,true),StructField(FBI Code,StringType,true),StructField(X Coordinate,StringType,true),StructField(Y Coordinate,StringType,true),StructField(Year,IntegerType,true),StructField(Updated On,TimestampType,true),StructField(Latitude,DoubleType,true),StructField(Longitude,DoubleType,true),StructField(Location,StringType,true)))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = StructType([StructField (x[0],x[1],x[2]) for x in labels])\n",
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Carregando o Schema**\n",
    "Aqui a gente pega o schema que foi criado e usa para carregar o CSV. Simples assim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Case Number: string (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Block: string (nullable = true)\n",
      " |-- IUCR: string (nullable = true)\n",
      " |-- Primary Type: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Location Description: string (nullable = true)\n",
      " |-- Arrest: boolean (nullable = true)\n",
      " |-- Domestic: boolean (nullable = true)\n",
      " |-- Beat: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- Ward: string (nullable = true)\n",
      " |-- Community Area: string (nullable = true)\n",
      " |-- FBI Code: string (nullable = true)\n",
      " |-- X Coordinate: string (nullable = true)\n",
      " |-- Y Coordinate: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Updated On: timestamp (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc_schema = spark.read.csv('reported-crimes.csv', schema = schema, header = True)\n",
    "rc_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Trabalhando com Colunas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Selecionando colunas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos acessar as colunas em dataframe dem Pyspark de duas maneiras:<p>\n",
    "Por indexing  -> **df['Column_name']**<br>\n",
    "Por função    -> **df.select(col('column_name'))** ou **df.select('column_name')**<p>\n",
    "É importante lembrar que se o nome da coluna tiver espaços ou nomes reservados você **não vai conseguir acessar** usando o acesso via atributo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|      ID|\n",
      "+--------+\n",
      "|10224738|\n",
      "+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.select(col('ID')).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+------+\n",
      "|      ID|               Date|Arrest|\n",
      "+--------+-------------------+------+\n",
      "|10224738|2015-09-05 13:30:00| false|\n",
      "+--------+-------------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.select('ID','Date','Arrest').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+------+\n",
      "|      ID|               Date|Arrest|\n",
      "+--------+-------------------+------+\n",
      "|10224738|2015-09-05 13:30:00| false|\n",
      "+--------+-------------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc['ID', 'Date','Arrest'].show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Trabalhando com os headers**\n",
    "Para retornar o header de um Dataframe em **Pyspark** fazemos igual ao **Pandas**.<br>\n",
    "E como sa saída é uma lista, podemos acessar essa lista via index ou usar outras ações aplicaveis a listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID', 'Case Number', 'Date']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vaocê tirar os brackets e testar as saídas\n",
    "rc.columns[0:3]\n",
    "#rc.columns\n",
    "#list(reversed(rc.columns))\n",
    "#rc.columns[::-1]\n",
    "#len(rc.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Adicionando novas colunas**\n",
    "\n",
    "Pandas -> **df['coluna_nova'] = df['coluna_velha'] * 2**<br>\n",
    "Pyspark -> **df.withColumn('coluna_nova', 2 * df['coluna_velha']**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|      ID|coluna_nova_2|\n",
      "+--------+-------------+\n",
      "|10224738|    5112369.0|\n",
      "|10224739|    5112369.5|\n",
      "+--------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc = rc.withColumn('coluna_nova_2', rc['ID'] / 2)\n",
    "rc.select('ID', 'coluna_nova_2').show(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Mudando os nomes das colunas**\n",
    "Pandas  -> **df.rename(columns={'Nome_antigo':'Nome_novo'})**<br>\n",
    "Pyspark -> **df.withColumnRenamed('Nome_antigo','Nome_Novo')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Case Number',\n",
       " 'Date',\n",
       " 'Block',\n",
       " 'IUCR',\n",
       " 'Primary Type',\n",
       " 'Description',\n",
       " 'Location Description',\n",
       " 'Arrest',\n",
       " 'Domestic',\n",
       " 'Beat',\n",
       " 'District',\n",
       " 'Ward',\n",
       " 'Community Area',\n",
       " 'FBI Code',\n",
       " 'X Coordinate',\n",
       " 'Y Coordinate',\n",
       " 'Year',\n",
       " 'Updated On',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'Location',\n",
       " 'IDx2']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc = rc.withColumnRenamed('coluna_nova_2','IDx2')\n",
    "rc.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Removendo colunas**\n",
    "\n",
    "Pyspark -> **df = df.drop('column')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Case Number',\n",
       " 'Date',\n",
       " 'Block',\n",
       " 'IUCR',\n",
       " 'Primary Type',\n",
       " 'Description',\n",
       " 'Location Description',\n",
       " 'Arrest',\n",
       " 'Domestic',\n",
       " 'Beat',\n",
       " 'District',\n",
       " 'Ward',\n",
       " 'Community Area',\n",
       " 'FBI Code',\n",
       " 'X Coordinate',\n",
       " 'Y Coordinate',\n",
       " 'Year',\n",
       " 'Updated On',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'Location']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc = rc.drop('IDx2')\n",
    "rc.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Trabalhando Filtros e Linhas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **filter()**\n",
    "#### Diferente do Pandas, onde podemos filtrar direto na seleção da coluna, ex: df['coluna' > 50], em pyspark nós usamos a função filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.filter(col('Date')  > '2017-11-11' ).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **distinct()**\n",
    "#### Selecionando valores únicos em um dataframe\n",
    "#### No pandas usamos **df['coluna'].unique()**, já aqui é um pouco diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select('Arrest').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **count()**\n",
    "#### Com ele contamos os valores selecionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prisoes = rc.filter(col('Arrest') == True).count() \n",
    "print(prisoes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **orderBy()**\n",
    "#### Usamos para fazer a ordenação do dataframe de acordo com a coluna selecionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|District|\n",
      "+--------+\n",
      "|     001|\n",
      "+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.filter(col('District') != 'null').select(col('District')).distinct().orderBy(col('District')).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **groupBy()**\n",
    "#### Usamos para agrupar valores de uma coluna específica e usar alguma agregação nesse resultado, como count(), sum(), entre outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|Arrest|  count|\n",
      "+------+-------+\n",
      "| false|4878870|\n",
      "|  true|1874411|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.groupBy('Arrest').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **union()**\n",
    "#### **Concatenando Dataframes**\n",
    "#### Seguindo as premissas já conhecidas de Python, Dataframes são imutaveis, deste modo não podemos fazer um append como fazemos com listas. Neste caso devemos concatenar os ataframes uns con os outros. \n",
    "#### **Critérios para concatenação**\n",
    "Os Dataframes devem ter o mesmo numero de colunas<br>\n",
    "Os Dataframes devem ter o mesmo schema\n",
    "#### No Pandas nós usamnos **pd.concat(df,df2)**, no Pyspark usamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para esse exemplo usaremso uma parte menor do Dataframe\n",
    "print(f\"Para o Distrito 008 temos {rc.filter(col('District') == '008').count()} registros\")\n",
    "print(f\"Para o Distrito 009 temos {rc.filter(col('District') == '009').count()} registros\")\n",
    "rc1 = rc.filter(col('District') == '008')\n",
    "rc2 = rc.filter(col('District') == '009')\n",
    "print(f\"Apos usar o union() o total de registros do novo dataframe é {rc1.union(rc2).count()}, que representa a soma dos totais anteriores.\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Desafios**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1) Quantos crimes resultaram em prisões?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Primeiro temos que entender o time de dados que estamos manipulando e suas variações.\n",
    "#Quantos valores possiveis temos para Arrest?\n",
    "'''\n",
    "rc.select(col('Arrest')).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Qual o tipo de dado?'''\n",
    "rc.printSchema()\n",
    "#Arrest: string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Agora fazemos a conta!'''\n",
    "result = round((rc.filter(col('Arrest') == 'true').count() / rc.select(col('Arrest')).count()) * 100,2) \n",
    "print(f'A porcentagem de crimes que resultaram em prisões é de {result}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1) Quais o TOP 3 de locais mais perigosos de Chigaco?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Primeiro vamos entender os dados. Temo um campo chamado Block (bairro)  que pode nos dar a resposta.\n",
    "#Mas antes algumas premissas devem ser definidas: O tipo de \"crime\" terá algum peso na resposta ou\n",
    "#apenas a quantidade de ocorrencias importa?\n",
    "É sempre importante fazer esse tipo de pergunta, mas nesse caso específico, vamos de quantidade.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos entender as informações que temos usando o **groupBy()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               Block|count|\n",
      "+--------------------+-----+\n",
      "|025XX N LAWNDALE AVE|  193|\n",
      "|   011XX S HOMAN AVE|  322|\n",
      "|010XX N LEAMINGTO...|  757|\n",
      "|    118XX S STATE ST|  600|\n",
      "|058XX W WAVELAND AVE|  121|\n",
      "| 015XX W HARRISON ST|   92|\n",
      "|018XX S CENTRAL P...|  728|\n",
      "| 001XX E DELAWARE PL|  907|\n",
      "|  043XX W GLADYS AVE|  923|\n",
      "| 096XX S CHAPPEL AVE|  184|\n",
      "|   030XX S KOLIN AVE|  146|\n",
      "|  059XX S SAWYER AVE|  180|\n",
      "|082XX S COMMERCIA...|  486|\n",
      "| 058XX N SHERIDAN RD| 1055|\n",
      "|059XX S NASHVILLE...|   20|\n",
      "|025XX W FULLERTON...|  337|\n",
      "|     006XX E 41ST ST|  261|\n",
      "|134XX S BALTIMORE...|  229|\n",
      "|     027XX E 78TH ST|  185|\n",
      "|     018XX W 54TH ST|   76|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.groupBy('Block').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora que temos a lista dos blocks com a contagem dos crimes agregada precisamos penas odernar de acordo com nossa necessodade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|             Block|count|\n",
      "+------------------+-----+\n",
      "|  100XX W OHARE ST|15243|\n",
      "|  001XX N STATE ST|12213|\n",
      "|076XX S CICERO AVE| 9229|\n",
      "+------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rc.groupBy('Block').count().orderBy('count', ascending = False).show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3) Quais o TOP 3 bairros com maior numero de prisoes efetuadas?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|             Block|count|\n",
      "+------------------+-----+\n",
      "|  001XX N STATE ST| 7143|\n",
      "|076XX S CICERO AVE| 4433|\n",
      "|  0000X N STATE ST| 4294|\n",
      "+------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Essa foi fácil. Agora que sabemso como agrupar as informações com o groupBy() e fazer as agregações, basta filtar as informações.\n",
    "Aqui vamos primeiro filtrar todos os registros onde o canpo Arrest é iguala  True. Depois só precisamos fazer as agregações.\n",
    "'''\n",
    "\n",
    "rc.filter(col('Arrest') == True).groupBy('Block').count().orderBy('count', ascending = False).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_environment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
